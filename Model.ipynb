{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759c1e51-8880-4265-9df9-fbdcd8c56740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(hp):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # Convolutional layers\n",
    "    for i in range(hp['num_layers']):\n",
    "        if i == 0:\n",
    "            model.add(layers.Conv2D(hp['filters'][i], hp['kernel_size'][i], activation='relu', input_shape=(48, 48, 3)))\n",
    "        else:\n",
    "            model.add(layers.Conv2D(hp['filters'][i], hp['kernel_size'][i], activation='relu'))\n",
    "        model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    # Dense layers\n",
    "    model.add(layers.Dense(hp['dense_units'], activation='relu'))\n",
    "    model.add(layers.Dense(6, activation='softmax'))  # Assuming 6 classes\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=hp['learning_rate']),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to optimize\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 64)\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "    filters = [trial.suggest_int(f'filters_{i}', 16, 128) for i in range(num_layers)]\n",
    "    kernel_size = [trial.suggest_categorical(f'kernel_size_{i}', [(3, 3), (5, 5)]) for i in range(num_layers)]\n",
    "    dense_units = trial.suggest_int('dense_units', 64, 512)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
    "    \n",
    "    epochs = 3 \n",
    "\n",
    "    \n",
    "    hp = {\n",
    "        'batch_size': batch_size,\n",
    "        'num_layers': num_layers,\n",
    "        'filters': filters,\n",
    "        'kernel_size': kernel_size,\n",
    "        'dense_units': dense_units,\n",
    "        'learning_rate': learning_rate,\n",
    "        'epochs': epochs\n",
    "    }\n",
    "\n",
    "   \n",
    "    model = create_model(hp)\n",
    "\n",
    "   \n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "   \n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=hp['epochs'],\n",
    "        batch_size=hp['batch_size'],\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "   \n",
    "    val_loss, val_acc = model.evaluate(val_ds)\n",
    "    return val_acc \n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Get the best trial\n",
    "best_trial = study.best_trial\n",
    "\n",
    "# Print the best trial results\n",
    "print(f\"Best trial parameters: {best_trial.params}\")\n",
    "print(f\"Best trial value: {best_trial.value}\")\n",
    "\n",
    "# Optional: Train the final model with the best hyperparameters\n",
    "best_hp = best_trial.params\n",
    "final_model = create_model(best_hp)\n",
    "final_model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=best_hp['epochs'],\n",
    "    batch_size=best_hp['batch_size']\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
